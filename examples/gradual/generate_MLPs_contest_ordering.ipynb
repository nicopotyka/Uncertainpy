{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This ipynb first generate MLP-like QBAFs and then compute the desired ordering."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step1: Generate random MLP-like QBAFs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "import sys\n",
    "sys.path.append(\"../../src/\")\n",
    "import uncertainpy.gradual as grad\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def generate_random_mlp_graph(layer_sizes, connection_prob):\n",
    "    \"\"\"\n",
    "    generate a random MLP-like QBAF structure, represented by DAG\n",
    "\n",
    "    parameter:\n",
    "    - connection_prob: The probability of inter-layer connections (1.0 = fully connected, 0.0 = no connections)\n",
    "\n",
    "    return:\n",
    "    - graph: 邻接表表示的 MLP 结构 (DAG)\n",
    "    \"\"\"\n",
    "\n",
    "    graph = {}  # Store the DAG using an adjacency list\n",
    "    node_id = 0  # neuron ID\n",
    "    layer_nodes = []  # Record the neuron IDs in each layer\n",
    "\n",
    "    # Create neuron nodes\n",
    "    for size in layer_sizes:\n",
    "        layer = [node_id + i for i in range(size)]\n",
    "        layer_nodes.append(layer)\n",
    "        node_id += size\n",
    "\n",
    "    # Generate inter-layer connections\n",
    "    for i in range(len(layer_nodes) - 1):  # Layer-by-layer connection\n",
    "        for src in layer_nodes[i]:  # Current layer neurons\n",
    "            for dst in layer_nodes[i+1]:  # Next layer neurons\n",
    "                if random.uniform(0, 1) < connection_prob:\n",
    "                    graph.setdefault(src, set()).add(dst)\n",
    "\n",
    "    # if node not in graph, add empty set\n",
    "    for layer in layer_nodes:\n",
    "        for node in layer:\n",
    "            graph.setdefault(node, set())\n",
    "\n",
    "    return graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_layer_nodes(layer_sizes, layer_index=None):\n",
    "    node_id = 0\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        if i == layer_index:\n",
    "            return [str(n) for n in range(node_id, node_id + size)]\n",
    "        node_id += size\n",
    "\n",
    "    raise ValueError(f\"Invalid layer_index {layer_index}: must be between 0 and {len(layer_sizes) - 1}\")\n",
    "\n",
    "# preferred_order = get_layer_nodes(layer_sizes, len(layer_sizes)-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# generate a random MLP-QBAF and output to a file\n",
    "def generate_and_write_graph(filename, layer_sizes, connection_prob):\n",
    "    with open(filename, 'w') as f:\n",
    "        sys.stdout = f\n",
    "\n",
    "        # generate the node and edge of a graph\n",
    "        random_graph = generate_random_mlp_graph(layer_sizes, connection_prob)\n",
    "\n",
    "        L0 = get_layer_nodes(layer_sizes, len(layer_sizes)-1)\n",
    "        L1 = get_layer_nodes(layer_sizes, len(layer_sizes)-2)\n",
    "        L2 = get_layer_nodes(layer_sizes, len(layer_sizes)-3)\n",
    "        L3 = get_layer_nodes(layer_sizes, len(layer_sizes)-4)\n",
    "\n",
    "        L0 = list(map(int, L0))\n",
    "        L1 = list(map(int, L1))\n",
    "        L2 = list(map(int, L2))\n",
    "        L3 = list(map(int, L3))\n",
    "\n",
    "        # generate random base scores for arguments\n",
    "        for node, edges in random_graph.items():\n",
    "            # print(node)\n",
    "            if node in L1:\n",
    "                random_float = round(random.uniform(0.0, 0.1),2)\n",
    "            else:\n",
    "                random_float = round(random.uniform(0.0, 1.0),2)\n",
    "            print(f\"arg({node}, {random_float}).\")\n",
    "\n",
    "        # generate random polarity for edges\n",
    "        for node, edges in random_graph.items():\n",
    "            for edge in edges:\n",
    "\n",
    "                if edge in L1:\n",
    "                    random_boolean = True\n",
    "                elif edge in L2:\n",
    "                    random_boolean = False\n",
    "                else:\n",
    "                    random_boolean = random.choice([True, False])\n",
    "                if random_boolean:\n",
    "                    print(f\"att({node}, {edge}).\")\n",
    "                else:\n",
    "                    print(f\"sup({node}, {edge}).\")\n",
    "\n",
    "    sys.stdout = sys.__stdout__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# set generation parameters\n",
    "connection_prob=1.0\n",
    "layer_sizes = [8,8,8,3]\n",
    "mlp_graph = generate_random_mlp_graph(layer_sizes, connection_prob)\n",
    "\n",
    "print(\"MLP structure:\", layer_sizes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: {8, 9, 10, 11, 12, 13, 14, 15},\n 1: {8, 9, 10, 11, 12, 13, 14, 15},\n 2: {8, 9, 10, 11, 12, 13, 14, 15},\n 3: {8, 9, 10, 11, 12, 13, 14, 15},\n 4: {8, 9, 10, 11, 12, 13, 14, 15},\n 5: {8, 9, 10, 11, 12, 13, 14, 15},\n 6: {8, 9, 10, 11, 12, 13, 14, 15},\n 7: {8, 9, 10, 11, 12, 13, 14, 15},\n 8: {16, 17, 18, 19, 20, 21, 22, 23},\n 9: {16, 17, 18, 19, 20, 21, 22, 23},\n 10: {16, 17, 18, 19, 20, 21, 22, 23},\n 11: {16, 17, 18, 19, 20, 21, 22, 23},\n 12: {16, 17, 18, 19, 20, 21, 22, 23},\n 13: {16, 17, 18, 19, 20, 21, 22, 23},\n 14: {16, 17, 18, 19, 20, 21, 22, 23},\n 15: {16, 17, 18, 19, 20, 21, 22, 23},\n 16: {24, 25, 26},\n 17: {24, 25, 26},\n 18: {24, 25, 26},\n 19: {24, 25, 26},\n 20: {24, 25, 26},\n 21: {24, 25, 26},\n 22: {24, 25, 26},\n 23: {24, 25, 26},\n 24: set(),\n 25: set(),\n 26: set()}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "N = 100 # generate N QBAFs storing in N files\n",
    "for i in range(N):\n",
    "    filename = f'../../bags/mlp_{i}.bag'\n",
    "    generate_and_write_graph(filename, layer_sizes, connection_prob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: computer desired orderings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# obtain a QBAF and set the gradual semantics\n",
    "\n",
    "# Computation via Forward Propagation\n",
    "bag = grad.BAG(\"../../bags/mlp_0.bag\")\n",
    "\n",
    "# # QE\n",
    "# agg_f = grad.semantics.modular.SumAggregation()\n",
    "# inf_f = grad.semantics.modular.QuadraticMaximumInfluence(conservativeness=1)\n",
    "\n",
    "# # DF-QuAD\n",
    "# agg_f = grad.semantics.modular.ProductAggregation()\n",
    "# inf_f = grad.semantics.modular.LinearInfluence(conservativeness=1)\n",
    "\n",
    "# # SD-DF-QUAD\n",
    "# agg_f = grad.semantics.modular.ProductAggregation()\n",
    "# inf_f = grad.semantics.modular.SQDFQUADInfluence(conservativeness=1)\n",
    "\n",
    "# EB\n",
    "agg_f = grad.semantics.modular.SumAggregation()\n",
    "inf_f = grad.semantics.modular.EulerBasedInfluence()\n",
    "\n",
    "# # EBT\n",
    "# agg_f = grad.semantics.modular.TopAggregation()\n",
    "# inf_f = grad.semantics.modular.EulerBasedInfluence()\n",
    "\n",
    "#returns dictionary of strength values if needed\n",
    "strength_values = grad.algorithms.computeStrengthValues(bag, agg_f, inf_f)\n",
    "\n",
    "for arg in bag.arguments.values():\n",
    "    print((arg.name,arg.strength))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_layer_nodes(layer_sizes, layer_index=None):\n",
    "    node_id = 0\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        if i == layer_index:\n",
    "            return [str(n) for n in range(node_id, node_id + size)]\n",
    "        node_id += size\n",
    "\n",
    "    raise ValueError(f\"Invalid layer_index {layer_index}: must be between 0 and {len(layer_sizes) - 1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['24', '25', '26']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_order = get_layer_nodes(layer_sizes, len(layer_sizes)-1)\n",
    "preferred_order"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "['16', '17', '18', '19', '20', '21', '22', '23']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immutable_args = get_layer_nodes(layer_sizes, len(layer_sizes)-2)\n",
    "immutable_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# # simple ReLU loss function (linear)\n",
    "# def compute_loss(bag, preferred_order):\n",
    "#     loss = 0\n",
    "#     for i in range(len(preferred_order) - 1):\n",
    "#         loss += max(0, (bag.arguments[preferred_order[i+1]].strength - bag.arguments[preferred_order[i]].strength))\n",
    "#     # print(f\"Loss: {loss}\")\n",
    "#     return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# pairwise smooth logistic loss (quadratic)\n",
    "def compute_loss(bag, preferred_order,delta=0):\n",
    "    loss = 0\n",
    "    # traverse all the pairs in preferred_order, i < j\n",
    "    for i in range(len(preferred_order) - 1):\n",
    "        for j in range(i + 1, len(preferred_order)):\n",
    "            A_i = preferred_order[i]\n",
    "            A_j = preferred_order[j]\n",
    "            # obtain strength σ(A_i) 和 σ(A_j)\n",
    "            sigma_i = bag.arguments[A_i].strength\n",
    "            sigma_j = bag.arguments[A_j].strength\n",
    "            loss += np.log(1 + np.exp(sigma_j - sigma_i + delta))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# compute gradient for the loss function\n",
    "def compute_gradient(h, bag, preferred_order):\n",
    "    # h is the perturbation score\n",
    "    gradient = {}\n",
    "    penalty = compute_loss(bag, preferred_order)\n",
    "    for arg in bag.arguments.values():\n",
    "\n",
    "        initial_weight = arg.get_initial_weight() # record initial base score\n",
    "        arg.reset_initial_weight(initial_weight+h) # perturb base scores\n",
    "        grad.algorithms.computeStrengthValues(bag, agg_f, inf_f) # recompute strength with new base score\n",
    "        # for arg in bag.arguments.values():\n",
    "        #     print((arg.name,arg.strength))\n",
    "\n",
    "        new_penalty = compute_loss(bag, preferred_order)\n",
    "        # print(new_penalty)\n",
    "\n",
    "        gradient[arg.name] = (new_penalty-penalty)/h # compute gradient\n",
    "        # print(f\"gradient for argument {arg.name} is {gradient[arg.name]}\")\n",
    "\n",
    "        arg.reset_initial_weight(initial_weight) # set it back to the original base score after computing gradient\n",
    "        grad.algorithms.computeStrengthValues(bag, agg_f, inf_f)\n",
    "        # for arg in bag.arguments.values():\n",
    "        #     print((arg.name,arg.strength))\n",
    "\n",
    "    return gradient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "{'0': -7.260414491838673e-07,\n '1': -7.260414491838673e-07,\n '2': -7.260414491838673e-07,\n '3': -7.260414491838673e-07,\n '4': -7.260414491838673e-07,\n '5': -7.260414491838673e-07,\n '6': -7.260414491838673e-07,\n '7': -7.260414491838673e-07,\n '8': -6.176392730594671e-07,\n '9': -1.4215295607300504e-07,\n '10': -1.3509193763638905e-07,\n '11': -6.920686246303375e-07,\n '12': -5.852651696613975e-07,\n '13': -2.6063595726100175e-07,\n '14': -2.3434587603787802e-07,\n '15': -2.2311041902867143e-07,\n '16': 0.0133789253009553,\n '17': 1.8582024807756168e-06,\n '18': 0.0133789253009553,\n '19': -0.0008423540087676428,\n '20': 0.000580033265862312,\n '21': 0.0008423539643587218,\n '22': 0.007497510612353152,\n '23': 0.000580033265862312,\n '24': -1.170547589834925,\n '25': 0.19770420425224697,\n '26': 0.9759826503419332}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 10e-6\n",
    "compute_gradient(h, bag, preferred_order)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# adam optimiser\n",
    "def adam_gradient(name, gradient, m, v, i):\n",
    "\n",
    "    grad = gradient[name]\n",
    "    # Adam optimiser parameters\n",
    "    learning_rate = 0.01  # Initial learning rate\n",
    "    beta1 = 0.9            # First-order moment decay rate\n",
    "    beta2 = 0.999          # Second-order moment decay rate\n",
    "    epsilon = 1e-8         # a small constant\n",
    "\n",
    "    # update first-order moment and second-order moment\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "\n",
    "    # bias correction\n",
    "    m_hat = m / (1 - beta1 ** i)\n",
    "    v_hat = v / (1 - beta2 ** i)\n",
    "\n",
    "    update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return update, m, v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# if the order is exactly the same as the desired order, then valid otherwise not.\n",
    "def is_valid_order(bag, preferred_order):\n",
    "    strengths = [bag.arguments[name].strength for name in preferred_order]\n",
    "    print(strengths)\n",
    "    return all(strengths[i] > strengths[i+1] for i in range(len(strengths)-1))\n",
    "print(is_valid_order(bag, preferred_order))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# test the validity for N MLP-like QBAFs\n",
    "def main():\n",
    "    valid = [0] * N\n",
    "    for i in tqdm(range(N)):\n",
    "        filename = f'../../bags/mlp_{i}.bag'\n",
    "        bag = grad.BAG(filename)\n",
    "        m = {}\n",
    "        v = {}\n",
    "        for epoch in range(1, 101):\n",
    "\n",
    "\n",
    "            # print(f\"Epoch:{epoch}\")\n",
    "            # compute gradient for all arguments\n",
    "            gradient = compute_gradient(10e-6, bag, preferred_order)\n",
    "            # print(f\"gradient:{gradient}\")\n",
    "            if all(value == 0 for value in gradient.values()): break\n",
    "\n",
    "            # update Adam state and update base scores\n",
    "            for arg in bag.arguments.values():\n",
    "                if arg.name not in immutable_args:\n",
    "                    if arg.name not in m:\n",
    "                        m[arg.name] = 0\n",
    "                        v[arg.name] = 0\n",
    "\n",
    "\n",
    "                    current_weight = arg.get_initial_weight()\n",
    "                    adam_update, m[arg.name], v[arg.name] = adam_gradient(arg.name, gradient, m[arg.name], v[arg.name], epoch)\n",
    "                    new_weight = current_weight - adam_update\n",
    "                    new_weight = max(0, min(1, new_weight))\n",
    "                    arg.reset_initial_weight(new_weight)\n",
    "\n",
    "\n",
    "            # recompute the strength and penalty\n",
    "            grad.algorithms.computeStrengthValues(bag, agg_f, inf_f)\n",
    "        if is_valid_order(bag, preferred_order):\n",
    "            valid[i] = 1\n",
    "    print(f\"valid:{valid}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
